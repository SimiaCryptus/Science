---
title: 'Neural Network Layer Analysis: MinkowskiRBFLayer'
layout: post
date: '"2025-11-29T00:00:00.000Z"'
last_modified: '"2025-11-28T14:19:56.000Z"'
category: projects
subcategory: Engineering & Formal Systems
tags:
  - Machine Learning
  - Spacetime
  - Technical Spec
keywords:
  - MinkowskiRBFLayer
  - neural network layer
  - spacetime
  - causal structure
  - timelike
  - spacelike
  - complex numbers
  - relativistic geometry
  - physics-informed machine learning
  - geometric deep learning
  - spatiotemporal modeling
  - causal inference
  - gradient analysis
  - numerical stability
  - pytorch
  - numpy
  - deep learning
status: working
last_thought_date: '"2025-11-28T00:00:00.000Z"'
thought_generation: 1
difficulty_level: research
reading_time_minutes: 25
document_type: framework
thinking_style: analytical
consciousness_level: meta
engagement_type: analytical
reader_participation: passive
cognitive_load: intense
description: >-
  Projects input vectors into Minkowski spacetime and computes complex-valued
  pseudo-distances to learned reference points, encoding causal structure
  through timelike vs. spacelike intervals.
excerpt: >-
  Projects input vectors into Minkowski spacetime and computes complex-valued
  pseudo-distances to learned reference points, encoding causal structure
  through timelike vs. spacelike intervals. By using the Minkowski metric from
  special relativity, this layer distinguishes between 'causally connected'
  (timelike) and 'causally disconnected' (spacelike) relationships, encoding
  this fundamental distinction in the real vs. imaginary components of the
  output.
featured_image: ../../assets/images/minowskirbf/minkowski_light_cone_diagram.png
og_image: ../../assets/images/minowskirbf/minkowski_light_cone_diagram.png
meta_title: 'MinkowskiRBFLayer: Neural Network for Causal Spacetime Relationships'
meta_description: >-
  Detailed analysis of MinkowskiRBFLayer, a novel neural network layer that
  projects inputs into Minkowski spacetime to encode causal relationships
  (timelike/spacelike) using complex-valued outputs. Covers theory,
  implementation, and stability.
meta_keywords: >-
  MinkowskiRBFLayer, neural network layer, spacetime, causal structure,
  timelike, spacelike, complex numbers, relativistic geometry, physics-informed
  machine learning, geometric deep learning, spatiotemporal modeling, causal
  inference, gradient analysis, numerical stability, pytorch, numpy, deep
  learning
og_title: 'MinkowskiRBFLayer: Causal Spacetime Neural Network'
og_description: >-
  Detailed analysis of MinkowskiRBFLayer, a novel neural network layer that
  projects inputs into Minkowski spacetime to encode causal relationships
  (timelike/spacelike) using complex-valued outputs. Covers theory,
  implementation, and stability.
og_type: article
og_locale: en_US
og_site_name: Fractal Thought Engine
schema_type: TechArticle
schema_headline: 'Neural Network Layer Analysis: MinkowskiRBFLayer'
schema_author: AI Assistant
schema_publisher: Fractal Thought Engine
schema_date_published: '"2025-11-29T00:00:00.000Z"'
schema_date_modified: '"2025-11-28T00:00:00.000Z"'
schema_image: /assets/images/minkowski_rbf_layer.png
schema_word_count: 5000
schema_reading_time: PT25M
canonical_url: >-
  https://fractalthoughtengine.com/projects/2025/11/29/nn-layer-20251128141956.html
robots: 'index,follow'
googlebot: 'index,follow'
bingbot: 'index,follow'
content_rating: general
content_language: en
is_featured: true
is_cornerstone: true
is_gateway: true
is_synthesis: true
related_documents:
  - _posts/projects/2025-11-27-nn-layer-20251127154041.md
  - _posts/phenomenology/2025-11-20-lorentzian-ethics-ai.md
  - _posts/learning/2025-07-06-probabilistic-neural-substrate.md
---

# Neural Network Layer Analysis: MinkowskiRBFLayer

**Started:** 2025-11-28 14:19:56

[See the Implementation](../../assets/nn_layer_20251128141956.html)

## Layer Specification

| Property | Value |
|----------|-------|
| Layer Name | MinkowskiRBFLayer |
| Input Shape | (N, D) |
| Output Shape | (N, M) |
| Activation | none |
| Analysis Depth | comprehensive |

## Forward Function Description

Projects N input vectors to (1+S)-dimensional Minkowski spacetime using a learned linear transformation, then computes pseudo-distances to M learned reference locations in this spacetime.

![**Figure 1: The Minkowski Light Cone.** The layer projects data into this geometry. Points inside the cone (Timelike) are causally connected to the reference point, while points outside (Spacelike) are causally disconnected. This distinction determines whether the output is Real or Imaginary.](../../assets/images/minowskirbf/minkowski_light_cone_diagram.png)


 The Minkowski metric (with signature -,+,+,...) separates the temporal and spatial components.

 The output is NxM complex numbers where: the real part encodes the sign of the spacetime interval (timelike vs spacelike), and the imaginary part encodes the magnitude.

Spatial dimensions are collapsed via the Minkowski metric: dsÂ² = -cÂ²dtÂ² + dxâ‚Â² + dxâ‚‚Â² + ... + dxâ‚›Â². 

For each input-reference pair, compute the spacetime interval, then output as complex: 

`z = sign(dsÂ²) * sqrt(|dsÂ²|)` when timelike (`dsÂ²<0`), or `i * sqrt(dsÂ²)` when spacelike (`dsÂ²>0`).

## Parameters

- W_proj: (D, 1+S) - Learned projection matrix mapping input features to (1+S)-dimensional Minkowski spacetime
- b_proj: (1+S,) - Bias for the projection transformation
- reference_points: (M, 1+S) - M learned reference locations in Minkowski spacetime
- c: (1,) - Learned or fixed speed of light parameter scaling the temporal dimension

---

# Executive Summary

## Projects input vectors into Minkowski spacetime and computes complex-valued pseudo-distances to learned reference points, encoding causal structure through timelike vs. spacelike intervals.

### Key Insight

> By using the Minkowski metric from special relativity, this layer distinguishes between 'causally connected' (timelike) and 'causally disconnected' (spacelike) relationships, encoding this fundamental distinction in the real vs. imaginary components of the output.

### Quick Decision Guide

| Aspect | Assessment |
|--------|------------|
| Computational Cost | medium |
| Training Difficulty | hard |
| Beginner Friendly | no |

### âœ… Strengths

- Naturally separates data based on causal structure (timelike vs. spacelike relationships)
- Complex output provides both sign and magnitude information in differentiable form
- Leverages relativistic geometry as physics-inspired inductive bias for temporal-spatial problems
- Adaptively discovers meaningful reference points in projected spacetime

### âš ï¸ Limitations

- Complex outputs require special handling in downstream layers
- Minkowski geometry is less intuitive than Euclidean space, reducing interpretability
- Hyperparameter sensitivity to spatial dimensions (S) and number of references (M)
- Potential gradient instabilities from sign discontinuities and square roots near zero
- Benefits unclear for problems without temporal or causal structure

### When to Use

Time-series data with causal dependencies, spatiotemporal modeling (event sequences, trajectories), problems where distinguishing temporal ordering from spatial separation matters, and physics-informed neural networks involving relativistic concepts.

### When NOT to Use

Standard classification/regression without temporal structure, applications requiring high interpretability, small datasets where added complexity isn't justified, or when downstream architecture cannot handle complex-valued features.


---

# Intuitive Explanation

## Real-World Analogy

 A cosmic GPS system that measures whether things can causally influence each other (like a lighthouse beam reaching ships), rather than just measuring distance. Some ships are reachable by light, others are forever separatedâ€”this layer distinguishes between 'causally connected' and 'causally disconnected' relationships.
![**Figure 3: The Causal Reach.** Like a lighthouse beam, the reference points in this layer can only "reach" inputs that fall within their light cone (timelike). Inputs outside the beam are physically separated (spacelike), a distinction this layer mathematically encodes.](../../assets/images/minowskirbf/cosmic_lighthouse_analogy.png)

 ## What Problem Does This Solve?

Traditional distance measures treat all directions equally, but many problems have asymmetric relationships where cause-and-effect, information flow, or hierarchical connections matter. This layer learns to recognize when two things can influence each other versus when they're forever separateâ€”distinguishing relationship type, not just strength.

## How Does It Work?

 Uses the Minkowski metric (borrowed from Einstein's physics) where the time component subtracts and space components add. Negative results (timelike) indicate causal connection and are encoded as real numbers; positive results (spacelike) indicate causal separation and are encoded as imaginary numbers. This preserves the crucial distinction between relationship types in a form downstream layers can use.
![**Figure 4: Encoding Causality in Complex Numbers.** The layer utilizes the complex plane to separate relationship types. Negative intervals (connected) become Real numbers, while positive intervals (separated) become Imaginary numbers, preserving the structural distinction for the neural network.](../../assets/images/minowskirbf/complex_plane_encoding_graph.png)

 ## Plain Language Walkthrough

Step 1: Project input data into a spacetime coordinate system with one time direction and multiple space directions. Step 2: Place learned reference beacons at fixed locations in this spacetime. Step 3: Measure distance using Minkowski metric where time subtracts and space adds. Step 4: Encode results as complex numbersâ€”real parts indicate timelike (connected) relationships, imaginary parts indicate spacelike (separated) relationships.

## Information Flow

 Input data â†’ Projection into spacetime â†’ Reference beacons scattered throughout â†’ Measure if each input is inside or outside the light cone of each beacon â†’ Output grid of complex numbers (real = connected, imaginary = separated). Picture a flashlight beam spreading from each reference point; inputs in the beam get real numbers, inputs outside get imaginary numbers.
![**Figure 2: Layer Architecture.** The input is projected into spacetime, where distances to reference points are calculated. The sign of the interval determines if the result is encoded in the real or imaginary component of the output.](../../assets/images/minowskirbf/minkowski_rbf_architecture_flow.png)

 ## Mental Model

Think of it as a 'Relationship Classifier with Built-in Physics.' The layer sorts pairs into two buckets: Real (could have met and influenced each other) and Imaginary (could never cross paths). It projects data onto a timeline and map, then uses the cosmic speed limit to classify relationships. By borrowing spacetime geometry, it naturally captures asymmetric, directional relationships that regular distance measures missâ€”giving neural networks an intuition for cause-and-effect baked into the mathematics.

## Understanding Gradients

Gradients adjust both the spacetime projection and reference beacon locations. If things that should be connected appear spacelike (imaginary), gradients push them closer in time. If things shouldn't be connected but appear timelike (real), gradients separate them spatially. Complex number outputs allow gradients to flow through both relationship type (real vs imaginary) and strength (magnitude), enabling fine-grained learning.

## âš ï¸ Common Misconceptions

- It's just measuring distance differentlyâ€”actually it measures a fundamentally different thing: relationship type, not distance
- Imaginary numbers are just a math trickâ€”they carry real information about spacelike relationships that downstream layers can use
- This only works for physics problemsâ€”it applies to any problem needing to distinguish between fundamentally different relationship types
- The time dimension must represent actual timeâ€”it's learned and might represent abstract concepts like causal priority or information flow direction


---

# Conceptual Diagram

## Layer Architecture

```
                              MinkowskiRBFLayer
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                                         â”‚
    â”‚  INPUT (N, D)                                                           â”‚
    â”‚      â”‚                                                                  â”‚
    â”‚      â–¼                                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  â”‚         LINEAR PROJECTION TO SPACETIME              â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   X_spacetime = X @ W_proj + b_proj                â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   W_proj: (D, 1+S)    b_proj: (1+S,)               â”‚               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚      â”‚                                                                  â”‚
    â”‚      â–¼                                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  â”‚         MINKOWSKI SPACETIME (1+S dims)              â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚    t (temporal)   xâ‚, xâ‚‚, ..., xâ‚› (spatial)        â”‚               â”‚
    â”‚  â”‚        â”‚                    â”‚                       â”‚               â”‚
    â”‚  â”‚   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                 â”‚               â”‚
    â”‚  â”‚   â”‚ -cÂ²dtÂ²  â”‚          â”‚ +dxÂ²    â”‚                 â”‚               â”‚
    â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚               â”‚
    â”‚  â”‚        â”‚                    â”‚                       â”‚               â”‚
    â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚               â”‚
    â”‚  â”‚                 â–¼                                   â”‚               â”‚
    â”‚  â”‚         dsÂ² = -cÂ²Î”tÂ² + Î£(Î”xáµ¢Â²)                     â”‚               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                    â”‚                                                    â”‚
    â”‚                    â–¼                                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  â”‚         REFERENCE POINTS (M, 1+S)                   â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚    â˜… refâ‚   â˜… refâ‚‚   â˜… refâ‚ƒ  ...  â˜… refâ‚˜          â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚    Each reference is a learned location in          â”‚               â”‚
    â”‚  â”‚    (1+S)-dimensional Minkowski spacetime            â”‚               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                    â”‚                                                    â”‚
    â”‚                    â–¼                                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  â”‚         SPACETIME INTERVAL COMPUTATION              â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   For each input point i and reference j:           â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   Î”t = t_input[i] - t_ref[j]                       â”‚               â”‚
    â”‚  â”‚   Î”x = x_input[i] - x_ref[j]  (S-dimensional)      â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   dsÂ²[i,j] = -cÂ² Ã— Î”tÂ² + ||Î”x||Â²                   â”‚               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                    â”‚                                                    â”‚
    â”‚                    â–¼                                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  â”‚         COMPLEX OUTPUT ENCODING                     â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚               â”‚
    â”‚  â”‚   â”‚   TIMELIKE      â”‚    â”‚   SPACELIKE     â”‚       â”‚               â”‚
    â”‚  â”‚   â”‚   dsÂ² < 0       â”‚    â”‚   dsÂ² > 0       â”‚       â”‚               â”‚
    â”‚  â”‚   â”‚                 â”‚    â”‚                 â”‚       â”‚               â”‚
    â”‚  â”‚   â”‚ z = -âˆš|dsÂ²|     â”‚    â”‚ z = iÃ—âˆš(dsÂ²)   â”‚       â”‚               â”‚
    â”‚  â”‚   â”‚ (real negative) â”‚    â”‚ (pure imaginary)â”‚       â”‚               â”‚
    â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚               â”‚
    â”‚  â”‚                                                     â”‚               â”‚
    â”‚  â”‚   Lightlike (dsÂ² = 0): z = 0                       â”‚               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                    â”‚                                                    â”‚
    â”‚                    â–¼                                                    â”‚
    â”‚              OUTPUT (N, M) âˆˆ â„‚                                          â”‚
    â”‚                                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

Stage 1 - Input Reception: N input vectors of dimension D enter the layer representing arbitrary feature vectors from the previous layer.

Stage 2 - Spacetime Projection: Each D-dimensional input is linearly projected to (1+S)-dimensional Minkowski spacetime using learned weights W_proj and bias b_proj.

The first dimension becomes the temporal coordinate (t) and remaining S dimensions become spatial coordinates (xâ‚, xâ‚‚, ... , xâ‚›).

Stage 3 - Reference Point Comparison: M learned reference points exist in the same Minkowski spacetime.

Each input point is compared against all M reference points creating an N Ã— M grid of pairwise comparisons.

Stage 4 - Minkowski Metric Computation: For each pair, compute the spacetime interval `dsÂ² = -cÂ²(Î”t)Â² + (Î”xâ‚)Â² + (Î”xâ‚‚)Â² + ... + (Î”xâ‚›)Â²`, where c (speed of light) controls relative scaling between temporal and spatial dimensions.

Stage 5 - Complex Encoding: Timelike intervals (dsÂ² < 0) output real negative numbers = `-âˆš|dsÂ²|`; Spacelike intervals (dsÂ² > 0) output pure imaginary numbers = iÃ—âˆš(dsÂ²); Lightlike intervals (dsÂ² = 0) output zero.

Stage 6 - Output: Final output is an N Ã— M complex tensor where real part encodes timelike distances and imaginary part encodes spacelike distances.


## Visual Flow Diagram

```mermaid
flowchart TB
    subgraph Input
        A[/"Input Tensor<br/>(N, D)"/]
    end
    
    subgraph Projection["Spacetime Projection"]
        B["Linear Transform<br/>W_proj: (D, 1+S)<br/>b_proj: (1+S)"]
        C[/"Spacetime Coords<br/>(N, 1+S)"/]
    end
    
    subgraph Spacetime["Minkowski Spacetime Structure"]
        D["Temporal Dim<br/>t (index 0)"]
        E["Spatial Dims<br/>xâ‚...xâ‚› (indices 1..S)"]
    end
    
    subgraph References["Learned References"]
        F[("Reference Points<br/>(M, 1+S)<br/>â˜… â˜… â˜… ... â˜…")]
    end
    
    subgraph Metric["Minkowski Metric Computation"]
        G["Compute Î”t, Î”x<br/>for all NÃ—M pairs"]
        H["dsÂ² = -cÂ²Î”tÂ² + ||Î”x||Â²<br/>c: speed of light param"]
    end
    
    subgraph Classification["Interval Classification"]
        I{"dsÂ² < 0?"}
        J["TIMELIKE<br/>Causal connection<br/>possible"]
        K["SPACELIKE<br/>No causal<br/>connection"]
        L["LIGHTLIKE<br/>On light cone"]
    end
    
    subgraph Encoding["Complex Output Encoding"]
        M["z = -âˆš|dsÂ²|<br/>(Real negative)"]
        N["z = iÃ—âˆš(dsÂ²)<br/>(Pure imaginary)"]
        O["z = 0"]
    end
    
    subgraph Output
        P[/"Output Tensor<br/>(N, M) âˆˆ â„‚"/]
    end
    
    A --> B
    B --> C
    C --> D
    C --> E
    D --> G
    E --> G
    F --> G
    G --> H
    H --> I
    I -->|"Yes (dsÂ² < 0)"| J
    I -->|"No (dsÂ² > 0)"| K
    I -->|"dsÂ² = 0"| L
    J --> M
    K --> N
    L --> O
    M --> P
    N --> P
    O --> P
    
    style A fill:#e1f5fe
    style P fill:#e8f5e9
    style F fill:#fff3e0
    style J fill:#ffcdd2
    style K fill:#c8e6c9
    style L fill:#fff9c4
```

## Parameter Roles

### W_proj

Projection Matrix with shape (D, 1+S). Learned parameter that maps D-dimensional input features into (1+S)-dimensional Minkowski spacetime, transforming feature space into relativistic spacetime coordinates.

### b_proj

Projection Bias with shape (1+S,). Learned parameter that translates the origin of the projected spacetime coordinates, allowing the layer to learn an offset for the spacetime embedding and center the data appropriately.

### reference_points

Spacetime Anchors with shape (M, 1+S). Learned parameter containing M fixed locations in Minkowski spacetime that serve as comparison anchors. Acts similarly to RBF centers but in a relativistic spacetime context. Each input is compared against all reference points.

### c

Speed of Light Parameter with shape (1,). Learned or fixed parameter that scales the temporal dimension relative to spatial dimensions. Controls the 'opening angle' of the light cone, determining the boundary between timelike and spacelike regions in the metric computation.



---

# Formal Definition

## Forward Function

$$\begin{align} \mathbf{P} &= \mathbf{X}\mathbf{W}_{\text{proj}} + \mathbf{1}_N \mathbf{b}_{\text{proj}}^\top \\ \Delta s^2_{nm} &= -c^2(P_{n,0} - R_{m,0})^2 + \sum_{k=1}^{S}(P_{n,k} - R_{m,k})^2 \\ z_{nm} &= \begin{cases} -\sqrt{-\Delta s^2_{nm}} & \Delta s^2_{nm} < 0 \\ i\sqrt{\Delta s^2_{nm}} & \Delta s^2_{nm} > 0 \\ 0 & \Delta s^2_{nm} = 0 \end{cases} \end{align}$$

**Notation:** `P = XÂ·W_proj + b_proj`; `Î”sÂ²_nm = -cÂ²(P_n0 - R_m0)Â² + Î£_k(P_nk - R_mk)Â²`; `z_nm = sgn(Î”sÂ²_nm)Â·âˆš|Î”sÂ²_nm|Â·ğŸ™(Î”sÂ²_nmâ‰¤0) + iÂ·âˆš(Î”sÂ²_nm)Â·ğŸ™(Î”sÂ²_nm>0)`

## Domain Constraints

- Input shape: X âˆˆ â„^(NÃ—D) with N â‰¥ 1, D â‰¥ 1
- Spatial dimensions: S â‰¥ 1 (typically S âˆˆ {1,2,3})
- Reference points: M â‰¥ 1
- Speed of light: c > 0 (strictly positive)
- Input values: X_nd âˆˆ â„ (unrestricted real numbers)
- Projection matrix: W_proj âˆˆ â„^(DÃ—(1+S))
- Projection bias: b_proj âˆˆ â„^(1+S)
- Reference points: R âˆˆ â„^(MÃ—(1+S))

## Range

Output Z âˆˆ â„‚^(NÃ—M) with codomain Z = â„â» âˆª {0} âˆª iâ„âº.
Timelike intervals (Î”sÂ² < 0) map to negative reals z âˆˆ (-âˆ,0); lightlike intervals (Î”sÂ² = 0) map to z = 0; spacelike intervals (Î”sÂ² > 0) map to positive imaginary z âˆˆ iâ„âº.
Geometric interpretation: Re(z) < 0 indicates causal connection (within light cone), Im(z) > 0 indicates causal disconnection (outside light cone), `|z|` represents proper distance/time magnitude.

## Parameter Initialization

- W_proj (Xavier/Glorot): U(-âˆš(6/(D+1+S)), âˆš(6/(D+1+S))) or N(0, 2/(D+1+S))
- b_proj: Initialize to zero vector, b_proj,k = 0 for all k
- Reference points R (Option A - Uniform): U(-Ïƒ_R, Ïƒ_R) where Ïƒ_R â‰ˆ 1 based on data scale
- Reference points R (Option B - K-means): Initialize from K-means clustering of projected training data
- Reference points R (Option C - Structured): Place on grid or vertices of regular polytope in spacetime
- Speed of light c (Fixed): c = 1 (natural units, recommended)
- Speed of light c (Learnable): c = softplus(c_raw) = log(1+exp(c_raw)) with c_raw ~ N(0, 0.1)


---

# Gradient Derivation (Backward Pass)

## Chain Rule Application

The backpropagation follows five sequential steps: (1) Gradient through complex square root with case distinction for timelike (Î”sÂ²<0, real output) vs spacelike (Î”sÂ²>0, imaginary output) separations; (2) Gradient through Minkowski metric tensor Î·=diag(-cÂ², 1, ..., 1) applied to coordinate differences; (3) Gradient w.r.t. reference points R using negative of the Minkowski gradient; (4) Gradient w.r.t. speed of light c from the metric coefficient; (5) Gradient through linear projection layer using standard matrix calculus. Each step applies the chain rule: dL/dX = (dL/dY) * (dY/dX).

## Gradient with Respect to Input

$\frac{\partial L}{\partial \mathbf{X}} = \frac{\partial L}{\partial \mathbf{P}} \mathbf{W}_{\text{proj}}^\top$

**Expression:** dL/dX = (dL/dP) @ W_proj.T, where dL/dP is computed through the Minkowski metric and complex square root operations

## Parameter Gradients

### âˆ‚L/âˆ‚W_proj

$\mathbf{X}^\top \frac{\partial L}{\partial \mathbf{P}}$

**Expression:** X.T @ (dL/dP), shape (D, 1+S)

### âˆ‚L/âˆ‚b_proj

$\mathbf{1}^\top \frac{\partial L}{\partial \mathbf{P}}$

**Expression:** sum(dL/dP, axis=0), shape (1+S,)

### âˆ‚L/âˆ‚R

$$-2 \sum_{n=1}^{N} \frac{\partial L}{\partial \Delta s^2_{nm}} \cdot \eta_{kk}(P_{n,k} - R_{m,k})$$

**Expression:** -2 * einsum('nm,nmk,k->mk', dL_ds2, delta, eta), shape (M, 1+S)

### âˆ‚L/âˆ‚c

$$-2c \sum_{n,m} \frac{\partial L}{\partial \Delta s^2_{nm}} (P_{n,0} - R_{m,0})^2$$

**Expression:** -2*c * sum(dL_ds2 * delta[:,:,0]**2), scalar

## Computational Graph

```mermaid
graph TD
    X[Input X] --> LP[Linear Projection<br>P = X*W_proj + b_proj]
    LP --> P[P]
    P --> MD["Minkowski DistanceÂ²<br>Î”sÂ² = -cÂ²(Pâ‚€-Râ‚€)Â² + Î£â‚–(Pâ‚–-Râ‚–)Â²"]
    MD --> DS[Î”sÂ²]
    DS --> CS["Complex Sqrt<br>z = sgn(Î”sÂ²)âˆš|Î”sÂ²| (timelike)<br>z = iâˆšÎ”sÂ² (spacelike)"]
    CS --> Z[Output z]

    subgraph Parameters
        W[W_proj]
        b[b_proj]
        R[Reference R]
        c[Speed of Light c]
    end

    W --> LP
    b --> LP
    R --> MD
    c --> MD
```

---

# Higher-Order Derivative Analysis

## Hessian Structure

Block-sparse structure with 4Ã—4 block organization: H = [H_WW, H_Wb, H_WR, H_Wc; H_bW, H_bb, H_bR, H_bc; H_RW, H_Rb, H_RR, H_Rc; H_cW, H_cb, H_cR, H_cc]. Dense coupling between W_proj and b_proj blocks; sparse/indirect coupling between (W,b) and (R,c) blocks through loss backpropagation. Block-diagonal structure in H_RR with M blocks of size (S+1)Ã—(S+1). Total dimension: D(S+1) + (S+1) + M(S+1) + 1.

## Eigenvalue Bounds

Unbounded near lightcone: `|âˆ‚Â²z_nm/âˆ‚P_nkâˆ‚P_nj| ~ |Î”sÂ²_nm|^(-1/2) â†’ âˆ as |Î”sÂ²_nm| â†’ 0`. 
Away from lightcone `(|Î”sÂ²_nm| â‰¥ Îµ > 0): Î»_max(H) â‰¤ Câ‚Îµ^(-1/2)â€–Xâ€–â‚‚Â² + Câ‚‚Îµ^(-3/2)â€–Xâ€–â‚‚â´; Î»_min(H) â‰¥ -Câ‚ƒÎµ^(-3/2)â€–Xâ€–â‚‚â´`. 
Condition number: `Îº(H) ~ O(â€–Xâ€–â‚‚â´/ÎµÂ²)`, worsening dramatically near lightcone.

## Second Derivatives

### minkowski_interval_second_derivative

`$âˆ‚Â²Î”sÂ²_nm/(âˆ‚P_nkâˆ‚P_nj) = 2Î·_kj`, where Î· = diag(-cÂ², 1, 1, ..., 1) is the Minkowski metric tensor$

### complex_distance_timelike

$$For Î”sÂ²_nm < 0: âˆ‚Â²z_nm/(âˆ‚P_nkâˆ‚P_nj) = Î·_kj/|z_nm| - (1/2|z_nm|Â³)(âˆ‚Î”sÂ²_nm/âˆ‚P_nk)(âˆ‚Î”sÂ²_nm/âˆ‚P_nj)$$

### complex_distance_spacelike

$$For Î”sÂ²_nm > 0: âˆ‚Â²z_nm/(âˆ‚P_nkâˆ‚P_nj) = iÎ·_kj/|z_nm| - (i/4(Î”sÂ²_nm)^(3/2))(âˆ‚Î”sÂ²_nm/âˆ‚P_nk)(âˆ‚Î”sÂ²_nm/âˆ‚P_nj)$$

### unified_form

$$âˆ‚Â²z_nm/(âˆ‚P_nkâˆ‚P_nj) = Î±_nmÂ·Î·_kj/|z_nm| - (Î±_nm/2|z_nm|Â³)(âˆ‚Î”sÂ²_nm/âˆ‚P_nk)(âˆ‚Î”sÂ²_nm/âˆ‚P_nj), 
where Î±_nm = 1 (timelike) or i (spacelike)$$

### projection_weight_hessian

$$H_WW = (X^T âŠ— I_(S+1)) H_PP (X âŠ— I_(S+1)), where H_PP is Hessian w.r.t. projections$$

### reference_point_hessian

$$âˆ‚Â²L/(âˆ‚R_mkâˆ‚R_m'k') = Î´_mm'[âˆ‘_n(âˆ‚Â²L/âˆ‚z_nmÂ²)(âˆ‚z_nm/âˆ‚R_mk)(âˆ‚z_nm/âˆ‚R_m'k') + (âˆ‚L/âˆ‚z_nm)(âˆ‚Â²z_nm/âˆ‚R_mkâˆ‚R_m'k')]$$

### speed_of_light_hessian

$$H_cc = âˆ‘_nm[(âˆ‚Â²L/âˆ‚z_nmÂ²)(âˆ‚z_nm/âˆ‚c)Â² + (âˆ‚L/âˆ‚z_nm)(âˆ‚Â²z_nm/âˆ‚cÂ²)], where âˆ‚Î”sÂ²_nm/âˆ‚c = -2c(P_n0 - R_m0)Â²$$

## Curvature Analysis

Inherent saddle point structure from Minkowski metric signature (-,+,+,...,+). 
Timelike regions (Î”sÂ² < 0): eigenvalues Î»_âˆ¥ = 1/|z_nm| - â€–g_nmâ€–Â²/(2|z_nm|Â³) along gradient direction; Î»_âŠ¥ = Î·_kk/|z_nm| perpendicular. 
Mixed curvature: negative in temporal direction (Î·â‚€â‚€ = -cÂ² < 0), positive in spatial directions (Î·_kk = +1 > 0). Spacelike regions exhibit complex-valued saddle structure. Lightcone (Î”sÂ² = 0) is degenerate singular point. Classification: timelike and spacelike regions are saddle points; lightcone is degenerate critical point.

## Fisher Information Matrix

Fisher Information Matrix F = J^T J where J = âˆ‚f/âˆ‚Î¸ is Jacobian. 
Block structure: F = [F_WW, F_WR, F_Wc; F_RW, F_RR, F_Rc; F_cW, F_cR, F_cc]. 
Key block: `F_WW = âˆ‘_n X_n^T X_n âŠ— (âˆ‘_m (âˆ‚z_nm/âˆ‚P_n)(âˆ‚z_nm^T/âˆ‚P_n))`. Critical singularity: `F ~ O(1/|Î”sÂ²|) â†’ âˆ` near lightcone. 
Geometric interpretation: high sensitivity region where small parameter changes cause large output changes. 
Condition number: `Îº(F) ~ O(max|z_nm|Â²/min|z_nm|Â²)`. Complex-valued gradients for spacelike intervals require `F_real = Re(J^H J)`.

## Natural Gradient Considerations

Natural gradient update: `Î¸_(t+1) = Î¸_t - Î· F^(-1) âˆ‡_Î¸ L`. 
Computational challenges: (1) Ill-conditioning `Îº(F) ~ O(max|z_nm|Â²/min|z_nm|Â²)`; (2) Complex-valued gradients requiring Wirtinger calculus for spacelike intervals; (3) Singularities near lightcone. Practical approximations: Kronecker-factored approximation (K-FAC): F_WW â‰ˆ E[X^T X] âŠ— E[(âˆ‚L/âˆ‚P)^T(âˆ‚L/âˆ‚P)]; Block-diagonal approximation: FÌƒ = blockdiag(F_WW, F_RR, F_cc). Recommended strategy: (1) Regularize |Î”sÂ²| â‰¥ Îµ to avoid singularities; (2) Separate learning rates for temporal (lr/cÂ²) vs spatial directions; (3) Use adaptive methods (Adam) with diagonal Fisher approximation; (4) Gradient clipping near lightcone with mask-based max_norm; (5) Curvature-aware learning rate: `Î·_adaptive = Î·â‚€/âˆš(1 + â€–Hâ€–_F/Ï„)`.

---

# Lyapunov Stability Analysis

## Lyapunov Function Candidate

$$V(Î¸) = L(Î¸) - L(Î¸*) â‰¥ 0$$ with augmented form `V_aug(Î¸) = L(Î¸) - L* + (Î¼/2)||Î¸ - Î¸*||Â²`.

Primary candidate uses loss difference from local minimum; augmented version adds quadratic regularization term for stronger guarantees on parameter manifold {W_proj, b_proj, R, c}.

## Stability Conditions

- Learning rate bound: Î· < 2/Î»_max(H) where H is the Hessian of loss function
- Lyapunov decrease: `Î”V = -Î·||âˆ‡L||Â² + (Î·Â²/2)âˆ‡L^T Hâˆ‡L < 0`
- Interval separation: `min_{n,m} |Î”sÂ²_{nm}(Î¸*)| > Î´ > 0` to avoid lightlike singularities
- Bounded speed of light: `0 < c_min â‰¤ c â‰¤ c_max < âˆ`
- Strict local minimum condition: `H* â‰» 0` (positive definite Hessian)
- Hessian smoothness: Lipschitz constant Î² bounds Hessian variation
- Strong convexity (local): Î¼-strong convexity with condition number `Îº = Î²/Î¼`

## Equilibrium Analysis

First-order conditions require `âˆ‡_Î¸ L = 0`. 
Critical equilibria classified as: (1) Strict local minima with H â‰» 0 (asymptotically stable), (2) Saddle points with indefinite H (unstable), (3) Degenerate minima with H â‰½ 0 singular (marginally stable). Projection weights satisfy `âˆ‡_{W_proj} L = X^T âˆ‚L/âˆ‚P = 0`; reference points satisfy `âˆ‘_n (âˆ‚L/âˆ‚z_{nm})(âˆ‚z_{nm}/âˆ‚Î”sÂ²)(âˆ‚Î”sÂ²/âˆ‚R_{m,k}) = 0`. 
Equilibrium stability depends on Hessian block structure across parameter blocks {W_proj, b_proj, R, c}.

## Basin of Attraction

Local basin for strict minimum Î¸* with `H* â‰» 0: B(Î¸*) âŠ‡ {Î¸ : ||Î¸ - Î¸*|| < 2Î»_min(H*)/Î²}`. 
Regime-dependent structure: Timelike region (Î”sÂ² < 0) has smooth gradients `âˆ (Î”sÂ²)^{-1/2}` with larger effective basin; Spacelike region (Î”sÂ² > 0) exhibits complex-valued phase dynamics with basin dependent on downstream loss handling; Lightlike boundary (Î”sÂ² â†’ 0) acts as separatrix with gradient singularity |âˆ‡z| â†’ âˆ. 
Basin size inversely proportional to Hessian Lipschitz constant Î² and directly proportional to minimum eigenvalue Î»_min(H*).

## Convergence Rate

Strongly convex case: `Ï = ((Îº-1)/(Îº+1))Â²` with `Îº = Î²/Î¼` condition number; optimal learning rate `Î· = 2/(Î¼+Î²)` achieves exponential convergence O(Ï^t). 
Non-convex case: gradient norm convergence `min_{tâ‰¤T} ||âˆ‡L(Î¸_t)||Â² â‰¤ 2(L(Î¸_0)-L*)/Î·T with O(1/T)` rate to stationary point. 
Regime-specific: Deep timelike `Îº_time ~ O(1)` yields fast convergence; Near lightlike Îº_light â†’ âˆ causes slow convergence/stalling; Deep spacelike Îº_space ~ O(1) fast if real-valued loss. Convergence bounded by `Î”V â‰¤ -Î·(1 - Î·Î»_max/2)||âˆ‡L||Â² < 0`.

## Potential Instability Modes

- âš ï¸ Exploding gradients at lightlike singularity: `|âˆ‚z_{nm}/âˆ‚Î”sÂ²| = 1/(2|Î”sÂ²|^{1/2}) â†’ âˆ as Î”sÂ² â†’ 0`; instability when `||W_proj|| Â· max_{n,m} |Î”sÂ²_{nm}|^{-1/2} > Î·^{-1}`
- âš ï¸ Vanishing gradients in large interval regime: `|âˆ‚z_{nm}/âˆ‚Î”sÂ²| ~ |Î”sÂ²|^{-1/2} â†’ 0 as |Î”sÂ²| â†’ âˆ`; occurs when `min_{n,m} |Î”sÂ²_{nm}| â‰« 1`
- âš ï¸ Speed of light runaway: Parameter c exhibits unique dynamics `âˆ‚Î”sÂ²/âˆ‚c = -2c(P_{n,0} - R_{m,0})Â²`; instability when c â†’ 0 (all intervals spacelike/spatial dominance) or c â†’ âˆ (all intervals timelike/temporal dominance)
- âš ï¸ Hessian indefiniteness: Saddle point dynamics with mixed positive/negative eigenvalues cause divergence from equilibrium
- âš ï¸ Gradient propagation amplification: `||âˆ‚L/âˆ‚X|| = ||âˆ‚L/âˆ‚P|| Â· ||W_proj||` can amplify upstream instabilities through projection layer
- âš ï¸ Degenerate Hessian singularity: Rank-deficient `H â‰½ 0` causes marginal stability with potential for slow drift away from equilibrium


---

# Lipschitz Continuity Analysis

## Forward Function Lipschitz Constant

Global: `L_forward = âˆ` (unbounded due to quadratic growth in Î”sÂ² and singularity at light cone). 
Local: $$L_forward^local â‰¤ Ïƒ_max(W_proj) Â· max(cÂ², 1) Â· D / âˆšÎµ$$ for bounded regions with `â€–P - Râ€–_max â‰¤ D and |Î”sÂ²| â‰¥ Îµ > 0`

## Gradient Lipschitz Constant (Smoothness)

Global: `Î² = âˆ` (not Lipschitz smooth). Local: `Î²^local â‰¤ Ïƒ_maxÂ²(W_proj) [max(cÂ²,1)/âˆšÎµ + DÂ² max(câ´,1)/(4Îµ^(3/2))]` for `|Î”sÂ²| â‰¥ Îµ`. 
Second derivative diverges as `|Î”sÂ²| â†’ 0` (light cone singularity)

## Spectral Norm Bounds

`â€–Jâ€–_2 â‰¤ [max(cÂ², 1) Â· D_max / (2âˆšÎµ_min)] Â· Ïƒ_max(W_proj)`, where `D_max = max_{n,m}â€–P_n - R_mâ€–` and `Îµ_min = min_{n,m}|Î”sÂ²_{nm}|`. 
Jacobian factors as diagonal Ã— sparse Ã— dense structure

## Gradient Flow Analysis

Exploding gradients near light cone `(âˆ‚z/âˆ‚(Î”sÂ²) â†’ âˆ as Î”sÂ² â†’ 0)`. Linear growth in far-field regions (âˆ D). 
Stable gradients in deep timelike/spacelike regions. Vanishing gradients less problematic due to sublinear square root growth. 
Requires gradient clipping and light cone regularization

## Smoothness Properties

- NOT globally Lipschitz continuous `(L = âˆ)`
- Locally Lipschitz away from light cone `(|Î”sÂ²| â‰¥ Îµ)`
- Differentiable everywhere except at light cone `(Î”sÂ² = 0)`
- CÂ¹ smooth away from light cone
- CÂ² smooth away from light cone
- NOT Î²-smooth globally; locally Î²-smooth for `|Î”sÂ²| â‰¥ Îµ`
- NOT convex (Minkowski metric is indefinite)
- Hessian of Î”sÂ² is constant with eigenvalues `{-2cÂ², 2, 2, ..., 2}`
- Second derivative of square root: `âˆ“1/(4|Î”sÂ²|^(3/2))` diverges at light cone
- Recommended modification: `z_soft = sign(Î”sÂ²) Â· âˆš(|Î”sÂ²| + Îµ)` achieves `L_soft = 1/(2âˆšÎµ)`


---

# Numerical Stability Analysis

## Overflow Conditions

- âš ï¸ Matrix multiplication in projection: `|X|âˆ Â· |W|âˆ Â· D > MAX_FLOAT`
- âš ï¸ Temporal component dominance: cÂ² Â· (Î”Pâ‚€)Â² when c â‰« 1
- âš ï¸ RBF kernel computation: `exp(-Î³|z|Â²)` when `Î³|z|Â² â‰« 1`
- âš ï¸ Gradient accumulation through square root near light-cone: `|âˆ‡z| â†’ âˆ as |Î”sÂ²| â†’ 0`

## Underflow Conditions

- âš ï¸ Small weights after initialization in projection layer
- âš ï¸ RBF kernel underflow: `K â†’ 0 when Î³|z|Â² â‰« 1`
- âš ï¸ Spatial gradient components when c â‰« 1 (temporal gradients dominate)

## Precision Recommendations

- Input X: float32 (standard)
- Projection P: float32 (matrix multiplication accumulation)
- Minkowski interval Î”sÂ² computation: float64 (catastrophic cancellation risk)
- Square root operation: float32 after stabilization
- Output z: float32/complex64 (standard)
- Use mixed precision with upcast to float64 for interval computation, downcast result to float32
- RBF kernel: compute in log-space to avoid overflow/underflow

## Stabilization Techniques

- âœ… Soft light-cone regularization: `z_stable = sign(Î”sÂ²) Â· âˆš(|Î”sÂ²| + Îµ)` where Îµ â‰ˆ 10â»â¶
- âœ… Log-space RBF computation: `log K = -Î³|Î”sÂ²|` with logsumexp for aggregation
- âœ… Running normalization of coordinates: `PÌƒ = (P - Î¼_P) / Ïƒ_P` to prevent large intervals
- âœ… Causal separation enforcement: `Î”sÂ²_safe = Î”sÂ² - Îµ_causal Â· sign(Î”sÂ²)` to push away from light-cone
- âœ… Xavier/He initialization scaled for spacetime dimension S+1
- âœ… Layer normalization applied to input X
- âœ… Component-wise gradient clipping for temporal vs spatial components
- âœ… Adaptive gradient clipping based on light-cone proximity

## Gradient Clipping

Component-wise clipping with separate thresholds: max_temporal=10.0, max_spatial=10.0. Implement adaptive clipping based on proximity to light-cone: adaptive_threshold = base_clip Â· âˆš(|Î”sÂ²| + 1e-6). Use training phase schedule: Warmup (0-10%) clip=0.1, Early (10-50%) clip=1.0, Late (50-100%) clip=5.0. Apply per-element clipping with proximity-aware thresholds to prevent gradient explosion near singularities.


---

# Reference Implementations

## PYTHON

### Dependencies

```python
import numpy as np
from typing import Tuple, Dict, Optional
```

### Forward Pass

```python
# Step 1: Project to Minkowski spacetime
P = X @ W_proj + b_proj

# Step 2: Compute spacetime intervals
temporal_diff = P[:, 0:1] - reference_points[:, 0]
spatial_diff = P[:, np.newaxis, 1:] - reference_points[np.newaxis, :, 1:]

# Compute Î”sÂ² using Minkowski metric
temporal_contrib = -c[0]**2 * temporal_diff**2
spatial_contrib = np.sum(spatial_diff**2, axis=2)
delta_s_squared = temporal_contrib + spatial_contrib

# Step 3: Compute z based on the sign of Î”sÂ²
z = np.zeros((N, M), dtype=np.complex128)

# Timelike: Î”sÂ² < 0 â†’ z = -âˆš(-Î”sÂ²)
timelike_mask = delta_s_squared < 0
z[timelike_mask] = -np.sqrt(-delta_s_squared[timelike_mask])

# Spacelike: Î”sÂ² > 0 â†’ z = iâˆš(Î”sÂ²)
spacelike_mask = delta_s_squared > 0
z[spacelike_mask] = 1j * np.sqrt(delta_s_squared[spacelike_mask])

# Cache for backward pass
cache = {
    'X': X, 'P': P, 'temporal_diff': temporal_diff,
    'spatial_diff': spatial_diff, 'delta_s_squared': delta_s_squared,
    'z': z, 'timelike_mask': timelike_mask, 'spacelike_mask': spacelike_mask
}
```

### Backward Pass

```python
# Compute âˆ‚z/âˆ‚(Î”sÂ²)
grad_delta_s_squared = np.zeros((N, M), dtype=np.float64)
eps = 1e-12

# Timelike case: z = -âˆš(-Î”sÂ²)
if np.any(timelike_mask):
    z_timelike = z[timelike_mask].real
    grad_z_timelike = grad_z[timelike_mask]
    dz_d_delta_s2 = 1.0 / (2.0 * z_timelike + eps * np.sign(z_timelike))
    grad_delta_s_squared[timelike_mask] = np.real(grad_z_timelike * dz_d_delta_s2)

# Spacelike case: z = iâˆš(Î”sÂ²)
if np.any(spacelike_mask):
    z_spacelike = z[spacelike_mask]
    grad_z_spacelike = grad_z[spacelike_mask]
    delta_s2_spacelike = delta_s_squared[spacelike_mask]
    dz_d_delta_s2 = z_spacelike / (2.0 * delta_s2_spacelike + eps)
    grad_delta_s_squared[spacelike_mask] = np.real(grad_z_spacelike * np.conj(dz_d_delta_s2))

# Gradient w.r.t. P
grad_P = np.zeros_like(P)
grad_P[:, 0] = np.sum(grad_delta_s_squared * (-2 * c[0]**2 * temporal_diff), axis=1)
for k in range(1, spacetime_dim):
    grad_P[:, k] = np.sum(grad_delta_s_squared * 2 * spatial_diff[:, :, k-1], axis=1)

# Gradient w.r.t. reference_points
grad_reference_points = np.zeros_like(reference_points)
grad_reference_points[:, 0] = np.sum(grad_delta_s_squared * (2 * c[0]**2 * temporal_diff), axis=0)
for k in range(1, spacetime_dim):
    grad_reference_points[:, k] = np.sum(
        grad_delta_s_squared[:, :, np.newaxis] * (-2 * spatial_diff[:, :, k-1:k]), axis=0
    ).squeeze()

# Gradient w.r.t. c
if learnable_c:
    grad_c = np.sum(grad_delta_s_squared * (-2 * c[0] * temporal_diff**2))
    grad_c = np.array([grad_c])
else:
    grad_c = np.zeros(1)

# Gradient w.r.t. W_proj and b_proj
grad_W_proj = X.T @ grad_P
grad_b_proj = np.sum(grad_P, axis=0)

# Gradient w.r.t. input X
grad_X = grad_P @ W_proj.T

grads = {
    'W_proj': grad_W_proj,
    'b_proj': grad_b_proj,
    'reference_points': grad_reference_points,
    'c': grad_c
}
```

### Initialization

```python
import numpy as np
from typing import Tuple, Dict, Optional

# Initialize parameters
if seed is not None:
    np.random.seed(seed)

# Xavier/Glorot initialization for projection weights
scale = np.sqrt(2.0 / (input_dim + spacetime_dim))
W_proj = np.random.randn(input_dim, spacetime_dim) * scale

# Zero initialization for bias
b_proj = np.zeros(spacetime_dim)

# Initialize reference points uniformly in a bounded region
reference_points = np.random.uniform(-1, 1, (num_reference_points, spacetime_dim))

# Speed of light parameter
c = np.array([c_value])
```

---

## PYTORCH

### Dependencies

```pytorch
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
```

### Forward Pass

```pytorch
# Project input to Minkowski spacetime
P = torch.matmul(X, self.W_proj) + self.b_proj  # (N, 1+S)

# Temporal differences: (N, M)
temporal_diff = P[:, 0:1] - self.reference_points[:, 0:1].T

# Spatial differences: (N, M, S)
spatial_diff = P[:, 1:].unsqueeze(1) - self.reference_points[:, 1:].unsqueeze(0)

# Compute Î”sÂ² with Minkowski metric (-,+,+,...)
temporal_contrib = -self.c**2 * temporal_diff**2  # (N, M)
spatial_contrib = torch.sum(spatial_diff**2, dim=-1)  # (N, M)
delta_s_squared = temporal_contrib + spatial_contrib  # (N, M)

# Compute z based on the sign of Î”sÂ²
z_real = torch.zeros_like(delta_s_squared)
z_imag = torch.zeros_like(delta_s_squared)

# Timelike intervals (Î”sÂ² < 0)
timelike_mask = delta_s_squared < -self.eps
z_real = torch.where(
    timelike_mask,
    -torch.sqrt(-delta_s_squared.clamp(max=-self.eps)),
    z_real
)

# Spacelike intervals (Î”sÂ² > 0)
spacelike_mask = delta_s_squared > self.eps
z_imag = torch.where(
    spacelike_mask,
    torch.sqrt(delta_s_squared.clamp(min=self.eps)),
    z_imag
)

# Combine into complex tensor
z = torch.complex(z_real, z_imag)

return z, delta_s_squared
```

### Backward Pass

```pytorch
# Gradient w.r.t. P (projected points)
# Temporal gradient: âˆ‚(Î”sÂ²)/âˆ‚Pâ‚€ = -2cÂ²(Pâ‚€ - Râ‚€)
grad_P_temporal = -2 * c**2 * temporal_diff  # (N, M)
grad_P_temporal = torch.sum(grad_delta_s_squared * grad_P_temporal, dim=1, keepdim=True)  # (N, 1)

# Spatial gradient: âˆ‚(Î”sÂ²)/âˆ‚Pâ‚– = 2(Pâ‚– - Râ‚–) for k > 0
grad_P_spatial = 2 * spatial_diff  # (N, M, S)
grad_P_spatial = torch.sum(
    grad_delta_s_squared.unsqueeze(-1) * grad_P_spatial, dim=1
)  # (N, S)

# Combine gradients for P
grad_P = torch.cat([grad_P_temporal, grad_P_spatial], dim=1)  # (N, 1+S)

# Gradient w.r.t. X: âˆ‚L/âˆ‚X = âˆ‚L/âˆ‚P @ W_proj.T
grad_X = torch.matmul(grad_P, W_proj.T)  # (N, D)

# Gradient w.r.t. W_proj: âˆ‚L/âˆ‚W_proj = X.T @ âˆ‚L/âˆ‚P
grad_W_proj = torch.matmul(X.T, grad_P)  # (D, 1+S)

# Gradient w.r.t. b_proj: âˆ‚L/âˆ‚b_proj = sum(âˆ‚L/âˆ‚P, dim=0)
grad_b_proj = torch.sum(grad_P, dim=0)  # (1+S,)

# Gradient w.r.t. reference_points R
grad_R_temporal = 2 * c**2 * temporal_diff  # (N, M)
grad_R_temporal = torch.sum(grad_delta_s_squared * grad_R_temporal, dim=0, keepdim=True).T  # (M, 1)

# Spatial: (M, S)
grad_R_spatial = -2 * spatial_diff  # (N, M, S)
grad_R_spatial = torch.sum(
    grad_delta_s_squared.unsqueeze(-1) * grad_R_spatial, dim=0
)  # (M, S)

grad_reference_points = torch.cat([grad_R_temporal, grad_R_spatial], dim=1)  # (M, 1+S)

# Gradient w.r.t. c: âˆ‚L/âˆ‚c = -2c Î£â‚™â‚˜ (âˆ‚L/âˆ‚Î”sÂ²)â‚™â‚˜ (Pâ‚™â‚€ - Râ‚˜â‚€)Â²
grad_c = -2 * c * torch.sum(grad_delta_s_squared * temporal_diff**2)
grad_c = grad_c.unsqueeze(0)  # (1,)

return grad_X, grad_W_proj, grad_b_proj, grad_reference_points, grad_c, None
```

### Initialization

```pytorch
# Xavier/Glorot initialization for projection matrix
nn.init.xavier_uniform_(self.W_proj)

# Zero initialization for bias
nn.init.zeros_(self.b_proj)

# Initialize reference points uniformly in a hypercube
nn.init.uniform_(self.reference_points, -1.0, 1.0)

# Speed of light parameter
if learnable_c:
    self.c = nn.Parameter(torch.tensor([c_init]))
else:
    self.register_buffer('c', torch.tensor([c_init]))
```

---


# Interactive Visualization Lab

[Launch the Interactive Lab](../../assets/nn_layer_20251128141956.html)

The accompanying HTML5/TensorFlow.js visualization provides a real-time environment to explore the dynamics of the MinkowskiRBFLayer. It projects a synthetic 3-class classification problem into a 1+1 dimensional spacetime (1 temporal, 1 spatial) to make the causal structure visually intuitive.

## Lab Components

### 1. Input Space
Displays the raw 2D input data. The data consists of three Gaussian clusters. In a standard neural network, these would be separated by hyperplanes. Here, they are prepared for projection into spacetime.

### 2. Spacetime Projection
This is the core visualization. It shows where the input points land in the learned Minkowski spacetime.
- **Vertical Axis:** Temporal dimension ($t$)
- **Horizontal Axis:** Spatial dimension ($x$)
- **Yellow Stars:** Learned reference points acting as "observers" or "events".
- **Light Cones:** Hovering over the canvas reveals the light cone structure. Points inside the cone are *timelike* separated (causally connected) from the cursor; points outside are *spacelike* separated.
- **Color Coding:** The visualization tints regions Cyan (Timelike) and Magenta (Spacelike) to show the causal horizon defined by the speed of light parameter $c$.

### 3. Layer Output
Visualizes the complex-valued activation for a selected reference point.
- **Cyan Glow:** Represents the magnitude of the real component (Timelike interval).
- **Magenta Glow:** Represents the magnitude of the imaginary component (Spacelike interval).
  This demonstrates how the layer effectively "slices" the input space into causal regions relative to each reference point.

## Key Experiments

### Varying the Speed of Light ($c$)
Using the slider to adjust $c$ changes the slope of the light cones ($slope = 1/c$).
- **High $c$:** The light cone widens. More points become timelike connected. The temporal dimension dominates the metric.
- **Low $c$:** The light cone narrows. Most points become spacelike separated. The spatial dimension dominates.
  *Observation:* Notice how the gradients struggle if $c$ is too extreme, as points get stuck in one causal regime.

### Training Dynamics
Clicking "Train (SGD)" runs a live optimization loop.
- Watch how the **Projection** view evolves. The network learns to rotate and scale the input clusters so they fall into specific causal relationships with the reference points.
- Often, the network will position reference points such that one class falls inside the light cone (timelike) while others fall outside (spacelike), effectively using the causal horizon as a decision boundary.

---

# Topological Analysis Lab: Minkowski Knots

[Launch the Knot Topology Lab](../../assets/2025-06-30-knots-lab.html)

This supplementary lab demonstrates the value of the Minkowski metric in a non-relativistic problem by using it as a metric with interesting topological properties. While this does not implement the RBF layer directly, it illustrates the geometric principles at play.

## Concept

In this visualization, the Minkowski metric is applied to the geometry of knots. In this case, crossings in traditional knot theory correspond to the topological categories expressed by Lorentz geometry. The "temporal" separation (or lack thereof) determines the topological category of the relationship between segments of the knot.

## Lab Features
- **3D Knot Generation**: Generate Trefoil, Figure-8, and Cinquefoil knots, or random splines.
- **Metric Switching**: Toggle between Euclidean and Minkowski metrics to see how the distance matrix transforms.
- **Causal Coloring**: The distance matrix visualizes the causal structure (Timelike vs Spacelike) between points on the knot, revealing hidden symmetries and crossing structures not visible in Euclidean space.
- **Physics Simulation**: Optimize the knot structure using forces derived from these metrics.

---

# Computational Complexity Analysis

## Time Complexity

| Pass | Complexity |
|------|------------|
| Forward | O(NÂ·DÂ·S + NÂ·MÂ·S) = O(NÂ·SÂ·(D + M)) |
| Backward | O(NÂ·DÂ·S + NÂ·MÂ·S) = O(NÂ·SÂ·(D + M)) |

## Space Complexity

O(NÂ·MÂ·S + NÂ·D) for activations; O(DÂ·S + MÂ·S) for parameters and gradients

## Memory Bandwidth

Memory-bound with low arithmetic intensity O(1). Forward: reads O(NÂ·D + NÂ·MÂ·S) bytes, writes O(NÂ·S + NÂ·M) bytes. Backward: reads all forward activations plus gradients, writes all gradients.

## Parallelization

Highly parallelizable: (1) GPU: Embarrassingly parallel over NÃ—M pairs with fused kernels using shared memory for reference points; branch divergence in conditional transform. (2) Distributed: Data parallelism (split N) recommended with AllReduce gradient sync O(DÂ·S + MÂ·S). Optimal CUDA: Grid (ceil(N/TILE_N), ceil(M/TILE_M)), Block (TILE_N, TILE_M). Recommendations: fuse kernels, use FP16/BF16, tile reference points, approximate for large M via LSH, checkpoint Î”sÂ² to save memory.


---

# Originality Analysis

## Novelty Assessment

HIGH ORIGINALITY - This layer represents a genuinely novel fusion of special relativity, kernel methods, and complex-valued neural networks. The use of Minkowski pseudo-metric to encode causal structure (timelike vs spacelike relationships) as complex-valued outputs is not found in existing literature. While related to hyperbolic neural networks and RBF networks, the indefinite signature approach creating categorical distinctions via complex numbers is fundamentally novel.

## Related Architectures

- Traditional Radial Basis Function Networks (Broomhead & Lowe, 1988)
- PoincarÃ© Embeddings (Nickel & Kiela, 2017)
- Hyperbolic Neural Networks (Ganea et al., 2018)
- Lorentz Model (Nickel & Kiela, 2018)
- Deep Complex Networks (Trabelsi et al., 2018)
- Complex-valued RBF networks (Chen et al.)
- Lorentz-equivariant Neural Networks (Bogatskiy et al., 2020)

## Key Innovations

- âœ¨ Causal structure as feature encoding: Using spacetime interval sign to distinguish timelike (real) vs spacelike (imaginary) relationships
- âœ¨ Complex output for metric signature: Real axis encodes timelike distances, imaginary axis encodes spacelike distances
- âœ¨ Learned spacetime projection: Network learns to map arbitrary features into pseudo-Riemannian manifold
- âœ¨ Learnable reference events: Reference points function as prototype events in spacetime for causal comparison

## Baseline Comparison

Standard RBF uses Euclidean metric with real positive outputs; Hyperbolic NNs use Riemannian geometry with real outputs; MinkowskiRBF uniquely uses pseudo-Riemannian Minkowski metric with complex outputs encoding causal relationship types. Only MinkowskiRBF explicitly encodes causal structure (timelike vs spacelike) in activations.

## Potential Research Contributions

- ğŸ“š Theoretical: Novel kernel in Minkowski space with complex-valued feature maps
- ğŸ“š Theoretical: First architecture explicitly encoding causal structure type in activations
- ğŸ“š Theoretical: Extension of geometric deep learning beyond Riemannian manifolds to pseudo-Riemannian spaces
- ğŸ“š Empirical: Potential excellence on temporal reasoning and event sequence modeling tasks
- ğŸ“š Empirical: Natural fit for hierarchical + temporal data combining hyperbolic and temporal structure benefits
- ğŸ“š Practical: Interpretable geometric meaning via real/imaginary split
- ğŸ“š Practical: Physics-inspired inductive bias for causal relationships

## Limitations

- âš ï¸ Computational complexity: Complex arithmetic adds overhead with O(NM(1+S)) distance computations
- âš ï¸ Gradient flow: Piecewise definition and square roots create optimization challenges near light cone (Î”sÂ² = 0)
- âš ï¸ Downstream compatibility: Requires complex-valued subsequent layers or explicit real/imaginary separation
- âš ï¸ Hyperparameter sensitivity: Speed of light c and spatial dimension S require careful tuning
- âš ï¸ Theoretical justification: Unclear why arbitrary learned features should obey Minkowski geometry; inductive bias may not match all domains
- âš ï¸ Baseline gap: No established baselines for this specific geometric approach complicates empirical validation


---

# Use Case Analysis

## Primary Application Domains

- ğŸ¯ Physics-Informed Machine Learning
- ğŸ¯ Spatiotemporal Modeling
- ğŸ¯ Geometric Deep Learning
- ğŸ¯ Causal Inference
- ğŸ¯ Relativistic Systems and Particle Physics

## Optimal Tasks

Tasks where this layer excels:

- âœ… Particle collision classification
- âœ… Causal event ordering
- âœ… Relativistic trajectory prediction
- âœ… Hyperbolic-like embeddings with causality
- âœ… Light cone classification
- âœ… Jet tagging in high-energy physics
- âœ… Event sequence modeling with causal constraints

## Unsuitable Tasks

Tasks where this layer may not be the best choice:

- âŒ Static image classification
- âŒ Standard NLP without discourse causality
- âŒ Tabular data without spacetime interpretation
- âŒ Language modeling
- âŒ General graph neural networks
- âŒ Recommender systems
- âŒ Audio classification with artificial spatial dimensions
- âŒ Low-latency inference applications

## Recommended Architectures

- ğŸ—ï¸ Minkowski Transformer for Event Sequences
- ğŸ—ï¸ Physics-Informed Encoder with DeepSets/Transformer aggregation
- ğŸ—ï¸ Causal Representation Learning with Real/Imaginary branch separation
- ğŸ—ï¸ Hybrid Geometric Network combining Euclidean and MinkowskiRBF branches

## Example Scenarios

### Scenario 1

High-Energy Physics Jet Tagging: Classify particle jets from detector 4-momenta using natural Minkowski structure and Lorentz invariance

### Scenario 2

Autonomous Vehicle Event Prediction: Predict object interactions using timelike/spacelike separation to determine causal contact possibility

### Scenario 3

Causal Discovery in Time Series: Discover causal relationships in multivariate temporal data using information propagation constraints

### Scenario 4

Video Event Understanding: Group spatiotemporal detections into causally connected clusters with physically plausible interaction speeds


## Integration Notes

Input preprocessing must normalize temporal and spatial scales appropriately, accounting for the speed-of-causality parameter c. Complex output requires careful handling: either concatenate real/imaginary components, use magnitude/phase decomposition, or employ complex-valued downstream layers. Gradient stability near zero requires epsilon clamping in sqrt operations. Initialize reference points to cover both timelike and spacelike regions of the light cone. Ensure deployment framework supports complex tensor operations. Validate that domain genuinely benefits from spacetime structure rather than forcing artificial interpretation onto Euclidean data.

## Scaling Considerations

Small problems (N<1K, M<100): direct computation on single GPU. Medium problems (N<100K, M<1K): batch processing with reference chunking. Large problems (N>100K, M>1K): approximate methods and reference subsampling. Memory complexity O(2Ã—NÃ—M) for complex output; computational complexity O(NÃ—MÃ—S) for distance computation. Implement chunked forward passes for large reference sets to manage memory efficiently.

## Industry Applications

- ğŸ­ Healthcare: Tumor growth modeling with spatial spread constraints, ICU patient deterioration event chains, epidemic modeling with propagation speed limits
- ğŸ­ Finance: High-frequency trading information propagation across exchanges, fraud detection through causal transaction chains, market contagion modeling
- ğŸ­ Autonomous Systems: Self-driving car collision prediction and interaction modeling, drone swarm coordination constraints, robotic reachability analysis
- ğŸ­ Scientific Computing: Particle physics jet classification and event reconstruction, astrophysics gravitational wave source localization, cosmology large-scale structure formation
- ğŸ­ Media & Entertainment: Video action causality and event sequencing, sports analytics play development and player interactions, physically plausible VFX/simulation


---

# Practical Guidance

## Hyperparameter Tuning

- Use differential learning rates: base_lr for spatial, base_lr/cÂ² for temporal, base_lr*0.1 for reference points, base_lr*0.01 for gamma
- Initialize temporal and spatial weights with scale sqrt(1/input_dim) to preserve input magnitude
- Initialize reference points on/near light cone (ref_temporal â‰ˆ norm(ref_spatial)/c) for well-behaved initial intervals
- Parameterize gamma in log-space (log_gamma parameter) and clamp exp(log_gamma) to [1e-4, 1e4] for stability
- Start with c=1.0 and adjust based on task; larger c emphasizes temporal component
- Use num_centers in range 16-256; balance expressivity against training difficulty
- Set spacetime_dim to 4 for physical interpretation or higher for abstract representations
- Use gradient clipping with max_norm=1.0 to prevent cÂ²-induced gradient explosion
- Enable mixed precision (AMP) but keep interval computation in float32 for numerical stability

## âš ï¸ Common Pitfalls

- Gradient explosion from cÂ² scaling in temporal terms: scale temporal learning rate by 1/cÂ² or use gradient clipping
- NaN from sqrt of negative intervals (spacelike): use complex output with real part for timelike, imaginary for spacelike
- Catastrophic cancellation near light cone: use stable formulation (a-b)(a+b)=aÂ²-bÂ² instead of direct subtraction
- Reference points collapsing to single location: add diversity regularization penalizing small pairwise Minkowski intervals
- Uniform learning rates causing temporal weights to explode: implement parameter group-specific learning rates
- Loss of precision in mixed precision training: keep interval_squared computation in float32
- Underflow when computing sqrt(abs_interval): add small epsilon (1e-7) before sqrt operation
- Lightlike intervals (â‰ˆ0) causing numerical issues: explicitly handle with separate real/imaginary encoding

## ğŸ”§ Debugging Tips

- Add debug mode to forward pass tracking: timelike_fraction, spacelike_fraction, lightlike_fraction, interval_range, weight norms
- Check gradient health after backward: detect NaN/Inf gradients, gradient explosion (>1e6), vanishing gradients (<1e-10)
- Monitor cÂ² scaling issues: temporal gradient norm should not exceed 100*cÂ² relative to spatial
- Visualize spacetime embedding: plot temporal vs spatial dimensions with reference points and light cone overlay
- Plot interval distribution histogram: should show mix of positive (timelike) and negative (spacelike) values
- Track reference point spread: std(reference_points) should remain non-zero to prevent collapse
- Verify output validity: check for NaN/Inf in complex output, ensure real and imaginary parts are bounded
- Use einsum operations for efficient batched computation and easier debugging of tensor shapes
- Log parameter norms separately for W_temporal, W_spatial, reference_points to identify which component is problematic

## âš¡ Performance Optimization

- Use einsum('bcd,d,bcd->bc', delta, metric, delta) for efficient batched Minkowski interval computation
- Process in chunks when num_centers is large (>1000) to reduce peak memory usage
- Use JIT compilation (@torch.jit.script) for inference-time kernel to eliminate Python overhead
- Implement memory-efficient forward pass processing reference points in chunks of size 1000
- Use torch.where for conditional operations instead of if statements to maintain differentiability
- Precompute metric tensor diag(cÂ², -1, -1, ...) as buffer to avoid recomputation
- Cache reference_points and gamma as buffers when exporting to ONNX
- Use numerically stable interval formulation: sign * |a-b| * (|a|+|b|) instead of aÂ²-bÂ²
- Batch normalize inputs before projection to spacetime to improve numerical stability

## ğŸ“Š Monitoring & Diagnostics

- Track timelike_ratio: should be balanced around 0.5; >0.95 indicates lack of expressivity, <0.05 suggests wrong c value
- Monitor gamma statistics: mean and std should remain in reasonable range (0.01-100); log-scale for visualization
- Log reference point spread: std(reference_points) per dimension; should not approach zero
- Track gradient norms per parameter group: temporal, spatial, reference_points, gamma should have different magnitudes
- Monitor interval_squared distribution: min/max/mean/std; should span both positive and negative ranges
- Check for distribution shift: compare input statistics against calibration baseline; alert if >10Ïƒ deviation
- Log loss components separately: reconstruction loss, diversity loss, regularization terms
- Use Weights & Biases dashboard with custom charts for c_value, gamma statistics, causal structure balance
- Implement alerts for extreme gamma values (>1000 or <0.001) indicating RBF degeneracy
- Track training stability: monitor for NaN/Inf in loss, gradient norms, and output activations

## ğŸš€ Production Best Practices

- Calibrate layer on representative data: compute input_mean and input_std for runtime validation
- Validate inputs at inference time: clamp extreme values to mean Â± 10*std from calibration
- Export to ONNX by wrapping complex output as separate real/imaginary tensors (ONNX doesn't support complex)
- Use JIT-compiled kernel for inference: eliminates Python overhead and enables graph optimization
- Run production_health_check before deployment: verify no NaN/Inf, output bounded, causal structure present
- Implement input distribution shift detection: warn when inputs deviate >10Ïƒ from calibration
- Store model with frozen parameters: use register_buffer for W_temporal, W_spatial, reference_points, gamma
- Document c value and spacetime_dim in model metadata for reproducibility
- Implement graceful degradation: fall back to simpler model if Minkowski layer produces invalid outputs
- Log inference metrics: output statistics, computation time, any warnings triggered by validation checks
- Version control calibration statistics: save input_mean/std with model checkpoint for consistency
- Use separate inference model class (MinkowskiRBFLayerProduction) with additional safeguards vs training class


---


---

## âœ… Analysis Complete

| Metric | Value |
|--------|-------|
| Total Time | 999s |
| Sections Generated | 15 |
| Implementation Languages | python, pytorch |

## Configuration Summary

| Setting | Value |
|---------|-------|
| Layer Name | MinkowskiRBFLayer |
| Input Shape | (N, D) |
| Output Shape | (N, M) |
| Activation | none |
| Analysis Depth | comprehensive |
| Higher-Order Analysis | true |
| Lyapunov Analysis | true |
| Lipschitz Analysis | true |
| Numerical Stability | true |
| Generate Tests | true |
